# âš¡ Real-Time ETL Data Platform

A real-time, event-driven data platform powered by **Apache Kafka**, **PySpark Structured Streaming**, and **Streamlit**. This project simulates an e-commerce order pipeline where events like "order received" â†’ "inventory deduction" â†’ "low stock alert" are handled in near real-time â€” from ingestion to transformation to visualization.

---

## ğŸš€ Project Motivation

As a data engineer, I wanted to go beyond batch pipelines and build something closer to production-grade **streaming data systems**. This project was an opportunity to:

- Learn Apache Kafka and PySpark with real-time streaming
- Simulate practical event chains (order â†’ inventory â†’ alert)
- Visualize streaming metrics via Streamlit
- Deepen system design thinking for portfolio + interviews

This system is designed for **learning, showcasing, and demoing** real-time data processing in action.

---

## âœ… Functional Requirements

- Ingest simulated orders as Kafka messages
- Use PySpark to stream, transform, and deduplicate events
- Write transformed data to local CSV (simulating data lake)
- Visualize order metrics and stock alerts in Streamlit
- Show event flows (new orders, inventory stock, low-stock items)

---

## ğŸ“¦ Non-Functional Requirements

- Lightweight, runs locally without external dependencies
- No database setup required (uses CSVs as sink)
- Modular code structure for easy expansion
- Uses virtual environment to isolate dependencies
- Handles large message bursts with structured streaming engine

---

## ğŸ§  System Design Overview

### Architecture

[ Kafka Producer (Python Script) ]
â†“
[ Kafka Topic: orders_topic ]
â†“
[ PySpark Structured Streaming Consumer ]
â†“
[ Transform & Write CSV Files ]
â†“
[ Streamlit Dashboard (Live Refresh) ]


---

## ğŸ“¸ Sneak Peek

### ğŸ”„ Kafka Producer

Simulates order events with random item + quantity every second.

### âš™ï¸ PySpark Consumer

Processes, filters, and appends events to CSV with low-stock alerts.

### ğŸ“Š Streamlit Dashboard

Real-time metrics including:
- ğŸ”¢ Orders per minute
- ğŸ“¦ Inventory left per item
- ğŸš¨ Low stock alerts

---

## ğŸ›  Local Setup

1. **Clone the repo**  

git clone https://github.com/jugalsheth/etl-realtime-platform.git
cd etl-realtime-platform
Set up virtual environment

python3 -m venv venv
source venv/bin/activate   # or venv\Scripts\activate on Windows

pip install -r requirements.txt
Start Kafka (if installed locally)
Make sure your Kafka server is running and a topic like orders_topic is created.


ğŸ’¡ Future Additions
Add Redis or Delta Lake as sink for better performance
Integrate alerting via email/Slack for low stock
Add REST API layer (FastAPI) to trigger new orders
Deploy dashboard + pipeline to cloud (e.g., AWS/GCP)
Build more event types (returns, payments, shipments)

ğŸ§‘â€ğŸ’» Built With
Apache Kafka
PySpark
Streamlit
Python
Pandas

Faker (for simulating data)

---

Screenshots
![Screenshot 2025-06-01 at 8 15 20â€¯PM](https://github.com/user-attachments/assets/32202339-b34d-4efc-9a8d-f39f0df71856)


ğŸ‘‹ Author
## ğŸ‘‹ Author **[Jugal Sheth](https://www.linkedin.com/in/jugalsheth/)** â€” Open to feedback, collabs, and any ideas to improve this!
